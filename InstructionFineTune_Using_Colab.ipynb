{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCfh5WjIxgBt"
      },
      "source": [
        "## Step 1 : Preparing the DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1_TKs0dxgBu",
        "outputId": "7f039967-b461-4484-d020-02f0e625c07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Content Preview:\n",
            " [\n",
            "    {\n",
            "        \"instruction\": \"Evaluate the following phrase by transforming it into the spelling given.\",\n",
            "        \"input\": \"freind --> friend\",\n",
            "        \"output\": \"The spelling of the given phrase \\\"\n",
            "Number of Entries: 1100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib.request\n",
        "import ssl\n",
        "\n",
        "# Define the file path in Colab's default directory\n",
        "file_path = \"/content/instruction-data.json\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.check_hostname = False\n",
        "    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    # Download only if the file doesn't exist\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "\n",
        "        # Debugging: Print first 200 characters to check the content\n",
        "        print(\"Downloaded Content Preview:\\n\", text_data[:200])\n",
        "\n",
        "        # Save the file\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "\n",
        "    # Load the JSON file\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        try:\n",
        "            data = json.load(file)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error: The file is not a valid JSON.\")\n",
        "            file.seek(0)\n",
        "            print(\"First few lines of the file:\\n\", file.read(200))\n",
        "            return None\n",
        "\n",
        "    return data\n",
        "\n",
        "# Run the function\n",
        "data = download_and_load_file(file_path, url)\n",
        "\n",
        "# Check if data was successfully loaded\n",
        "if data:\n",
        "    print(\"Number of Entries:\", len(data))\n",
        "else:\n",
        "    print(\"Failed to load JSON. Check the downloaded content.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxvttlssxgBw",
        "outputId": "5f3ceba6-cbf3-4177-c631-0d42e8105b6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Entry :  {'instruction': 'Convert this sentence to passive voice', 'input': 'The chef cooked a delicious meal.', 'output': 'A delicious meal was cooked by the chef.'}\n"
          ]
        }
      ],
      "source": [
        "print( \"Example Entry : \" , data[24])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yxl20z5ixgBw"
      },
      "outputs": [],
      "source": [
        "# We will use the Alpha Prompt Fromat for fine tuning using the json file\n",
        "#-> Called Aplaca Style\n",
        "\n",
        "#converting the instructions into Alpha Format\n",
        "def format_input( entry ):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task.\"\n",
        "        f\" Write a response that appropiately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{ entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QyceW-MxgBw",
        "outputId": "a78f2b3d-ef28-47ea-959f-ea6266ab1588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropiately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "### Response:\n",
            "An antonym of 'complicated' is 'simple'.\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input( data[999])\n",
        "\n",
        "desired_reponse = f\"\\n\\n### Response:\\n{ data[999]['output']}\"\n",
        "print( model_input + desired_reponse )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n5XJvLTLxgBx"
      },
      "outputs": [],
      "source": [
        "#Spliting the Data Set for Train-Test-Validation\n",
        "train_portion = int(len( data ) * 0.85 )\n",
        "test_portion = int( len( data ) * 0.1 )\n",
        "val_portion =  len( data ) - train_portion - test_portion\n",
        "\n",
        "train_data = data[:train_portion ]\n",
        "test_data = data[ train_portion : train_portion + test_portion ]\n",
        "val_data= data[ train_portion + test_portion :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8r68otLxgBx",
        "outputId": "58a11c45-e53f-4c7a-b4c9-6dd3ae8bb869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length :  935\n",
            "validation set length :  55\n",
            "Test set length :  110\n"
          ]
        }
      ],
      "source": [
        "print(\"Training set length : \" , len( train_data ))\n",
        "print(\"validation set length : \" , len( val_data ))\n",
        "print(\"Test set length : \" , len( test_data ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7e9WWrxxgBy"
      },
      "source": [
        "## Batching the DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DJUqIqNUxgBy"
      },
      "outputs": [],
      "source": [
        "# Converting the whole input to token ids\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset( Dataset ):\n",
        "    def __init__( self , data , tokenizer ):\n",
        "        self.data = data\n",
        "\n",
        "        #pre tokeinize text\n",
        "        self.encoded_texts = []\n",
        "        for entry in data :\n",
        "            instruction_plus_input = format_input( entry )\n",
        "            response_text = f\"\\n\\n### Response:\\n{ entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append( tokenizer.encode( full_text ))\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[ index ]\n",
        "\n",
        "    def __len__( self ):\n",
        "        return len( self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFdQqusNxgBz",
        "outputId": "eecd1fc0-953a-41c3-d47e-bc4c6a73666f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.2 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n",
            "[50256]\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print( tokenizer.encode(\"<|endoftext|>\" , allowed_special= {\"<|endoftext|>\"}) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjeZnTbZxgBz"
      },
      "source": [
        "Step 1 : Find the longest input length in the batch.\n",
        "\n",
        "Step 2 : Pad and prepare the inputs.\n",
        "\n",
        "Step 3 : Remove the extra padded token added earlier.\n",
        "\n",
        "Step 4 : convert the list of input to tensor and transfer to target device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AIpJQw0yxgBz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def custom_collate_draft_1(  batch , pad_toke_id = 50256 , device = \"cpu\" ):\n",
        "    # Find the longest length in the batch and add it by 1\n",
        "\n",
        "    batch_max_length = max( len( item ) + 1 for item in batch )\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    input_lst = []\n",
        "\n",
        "\n",
        "    for item in batch :\n",
        "        new_item = item.copy()\n",
        "        new_item += [pad_toke_id ]\n",
        "        padded = (\n",
        "            new_item + [ pad_toke_id ] * ( batch_max_length - len( new_item ))\n",
        "\n",
        "        )\n",
        "        inputs = torch.tensor( padded[: -1 ])\n",
        "        input_lst.append( inputs )\n",
        "\n",
        "    # convert the input list to tensor and transfer to target\n",
        "    inputs_tensor = torch.stack( input_lst ).to( device )\n",
        "    return inputs_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzK8vF-3xgBz",
        "outputId": "9dbe5a33-6005-4147-d8bb-126d2d83bf54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [ 0, 1 , 2, 3, 4 ]\n",
        "inputs_2 = [ 5, 6 ]\n",
        "inputs_3 = [ 7 , 8 , 9 ]\n",
        "\n",
        "batch = (\n",
        "    inputs_1 , inputs_2 , inputs_3\n",
        ")\n",
        "print( custom_collate_draft_1( batch ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JdDsSZHxgBz"
      },
      "source": [
        "### Creating the Token Ids for targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "medXMs7vxgBz"
      },
      "outputs": [],
      "source": [
        "def custom_collate_draft_2( batch , pad_toke_id = 50256 , device = \"cpu\" ):\n",
        "    # Find the longest length in the batch and add it by 1\n",
        "\n",
        "    batch_max_length = max( len( item ) + 1 for item in batch )\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    input_lst, target_lst  = [], []\n",
        "\n",
        "\n",
        "    for item in batch :\n",
        "        new_item = item.copy()\n",
        "        new_item += [pad_toke_id ]\n",
        "        padded = (\n",
        "            new_item + [ pad_toke_id ] * ( batch_max_length - len( new_item ))\n",
        "\n",
        "        )\n",
        "        inputs = torch.tensor( padded[: -1 ])\n",
        "        targets = torch.tensor( padded[ 1 : ])\n",
        "        input_lst.append( inputs )\n",
        "        target_lst.append( targets )\n",
        "\n",
        "    #Convert the List of inputs to tensor and transfer to target device\n",
        "    input_tensor = torch.stack( input_lst ).to( device )\n",
        "    target_tensor = torch.stack( target_lst ).to( device )\n",
        "    return input_tensor , target_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRZsPUWxxgBz",
        "outputId": "8d716b3f-95df-40c9-d097-0a25c6f9a002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256, 50256, 50256, 50256],\n",
            "        [    8,     9, 50256, 50256, 50256]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [ 0, 1 , 2, 3, 4 ]\n",
        "inputs_2 = [ 5, 6 ]\n",
        "inputs_3 = [ 7 , 8 , 9 ]\n",
        "\n",
        "batch = (\n",
        "    inputs_1 , inputs_2 , inputs_3\n",
        ")\n",
        "inputs , targets = custom_collate_draft_2( batch )\n",
        "print( inputs )\n",
        "print( targets )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vIDNlWR3xgBz"
      },
      "outputs": [],
      "source": [
        "#In the Next Step we will replace Padding token with -100 so that it does not contribute in the training loss\n",
        "#but retaining the one 505256 token as the mark of end of text token\n",
        "\n",
        "\n",
        "def custom_collate_fn( batch , pad_token_id = 50256 , ignore_index = -100 , allowed_max_length = None , device = \"cpu\" ):\n",
        "    # Find the longest length in the batch and add it by 1\n",
        "\n",
        "    batch_max_length = max( len( item ) + 1 for item in batch )\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    input_lst, target_lst  = [], []\n",
        "\n",
        "\n",
        "    for item in batch :\n",
        "        new_item = item.copy()\n",
        "        new_item += [pad_token_id ]\n",
        "        padded = (\n",
        "            new_item + [ pad_token_id ] * ( batch_max_length - len( new_item ))\n",
        "\n",
        "        )\n",
        "        inputs = torch.tensor( padded[: -1 ])\n",
        "        targets = torch.tensor( padded[ 1 : ])\n",
        "\n",
        "\n",
        "        #Replace all but the first padding token with the ignore index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[ indices[1 : ]] = ignore_index\n",
        "\n",
        "\n",
        "        # Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None :\n",
        "            inputs = inputs[ : allowed_max_length ]\n",
        "            targets = targets[: allowed_max_length ]\n",
        "\n",
        "        input_lst.append( inputs )\n",
        "        target_lst.append( targets )\n",
        "\n",
        "\n",
        "    #Convert the List of inputs to tensor and transfer to target device\n",
        "    input_tensor = torch.stack( input_lst ).to( device )\n",
        "    target_tensor = torch.stack( target_lst ).to( device )\n",
        "    return input_tensor , target_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJIBJgYaxgB0",
        "outputId": "0685a1b6-e437-4f44-b81b-06f0960149cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [ 0, 1 , 2, 3, 4 ]\n",
        "inputs_2 = [ 5, 6 ]\n",
        "inputs_3 = [ 7 , 8 , 9 ]\n",
        "\n",
        "batch = (\n",
        "    inputs_1 , inputs_2 , inputs_3\n",
        ")\n",
        "inputs , targets = custom_collate_fn( batch )\n",
        "print( inputs )\n",
        "print( targets )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWX1o9KfxgB0",
        "outputId": "4050fba8-dc46-45b9-fe84-15bd25050c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n"
          ]
        }
      ],
      "source": [
        "#The prupose of doing the target paddimg token of 50256 with -100\n",
        "# The Demo for that\n",
        "logits_1 = torch.tensor(\n",
        "    [[ -1.0 , 1.0 ],\n",
        "     [ -0.5 , 1.5 ]]\n",
        ")\n",
        "targets_1 = torch.tensor( [ 0 , 1 ])\n",
        "loss_1 = torch.nn.functional.cross_entropy( logits_1 , targets_1 )\n",
        "print( loss_1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R3pNqF-xgB0",
        "outputId": "85e94972-42fc-434d-fac1-83d3a060c32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7936)\n"
          ]
        }
      ],
      "source": [
        "logits_2 = torch.tensor(\n",
        "    [[ -1.0 , 1.0 ],\n",
        "     [ -0.5 , 1.5 ],\n",
        "     [ -0.5 , 1.5 ]]\n",
        ")\n",
        "targets_2 = torch.tensor( [ 0 , 1, 1 ])\n",
        "loss_2 = torch.nn.functional.cross_entropy( logits_2 , targets_2 )\n",
        "print( loss_2 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHS1tPJbxgB0",
        "outputId": "5ad3efd6-8bd0-4909-8ed0-aae1822e1478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n",
            "Loss 1 == Loss 3  tensor(True)\n"
          ]
        }
      ],
      "source": [
        "targets_3 = torch.tensor( [ 0 , 1 , -100 ])\n",
        "\n",
        "loss_3 = torch.nn.functional.cross_entropy( logits_2 , targets_3 )\n",
        "\n",
        "print( loss_3 )\n",
        "print( \"Loss 1 == Loss 3 \" , loss_1 == loss_3 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T74uzrfExgB0"
      },
      "source": [
        "## Creating the Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "125tgE6vxgB0",
        "outputId": "98061b1c-bb74-414f-974c-d9430c30ac8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device :  cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device( \"mps\")\n",
        "print( \"Device : \", device )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IwPuPe61xgB0"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "custom_collate_fn = partial( custom_collate_fn , device = device , allowed_max_length = 1024 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hsolhkg8xgB0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed( 123 )\n",
        "\n",
        "train_dataset = InstructionDataset( train_data , tokenizer )\n",
        "train_loader = DataLoader(\n",
        "    train_dataset ,\n",
        "    batch_size = batch_size ,\n",
        "    collate_fn= custom_collate_fn,\n",
        "    shuffle= True ,\n",
        "    drop_last= True,\n",
        "    num_workers= num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset( val_data , tokenizer )\n",
        "val_loader = DataLoader(\n",
        "    val_dataset ,\n",
        "    batch_size = batch_size ,\n",
        "    collate_fn= custom_collate_fn ,\n",
        "    shuffle= False ,\n",
        "    drop_last= False ,\n",
        "    num_workers= num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset( test_data , tokenizer )\n",
        "test_loader = DataLoader(\n",
        "    test_dataset ,\n",
        "    batch_size = batch_size ,\n",
        "    collate_fn= custom_collate_fn ,\n",
        "    shuffle= False ,\n",
        "    drop_last= False ,\n",
        "    num_workers= num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxBb7c-4xgB0",
        "outputId": "e2c12930-2db2-40d8-c812-4503a160905d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loader : \n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 93]) torch.Size([8, 93])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Loader : \")\n",
        "for inputs , targets in train_loader :\n",
        "    print( inputs.shape , targets.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NheefPYxgB1"
      },
      "source": [
        "## Loading The Pretrain Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DAMQF2T6xgB1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "### Layer Normalisation and Feed Forward Neural Network\n",
        "class LayerNorm( nn.Module ):\n",
        "    def __init__( self , emb_dim ):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter( torch.ones( emb_dim ))\n",
        "        self.shift = nn.Parameter( torch.zeros( emb_dim ))\n",
        "\n",
        "\n",
        "    def forward( self , x ):\n",
        "        mean = x.mean( dim = -1 , keepdim = True )\n",
        "        var = x.var( dim = -1 , keepdim = True , unbiased = True )\n",
        "        norm_x = ( x - mean ) / torch.sqrt( var + self.eps )\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU( nn.Module ):\n",
        "    def __init__( self  ):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward( self , x ):\n",
        "        return 0.5 * x * ( 1 + torch.tanh(\n",
        "            torch.sqrt( torch.tensor( 2.0 / torch.pi )) *\n",
        "            ( x + 0.044715 * torch.pow( x , 3 ))\n",
        "        ))\n",
        "\n",
        "class FeedForward( nn.Module ):\n",
        "    def __init__( self , cfg ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear( cfg[\"emb_dim\" ] , 4 * cfg[\"emb_dim\" ] ),\n",
        "            GELU(),\n",
        "            nn.Linear( 4 * cfg[ \"emb_dim\"] , cfg[ \"emb_dim\" ]),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward( self , x ):\n",
        "        return self.layers( x )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Nq6aGyAnxgB1"
      },
      "outputs": [],
      "source": [
        "### MutiHead Attention Mech\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "i7A9eD1XxgB2"
      },
      "outputs": [],
      "source": [
        "### Transformer Class\n",
        "class TransformerBlock( nn.Module ):\n",
        "    def __init__( self , cfg ):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in = cfg[ \"emb_dim\"],\n",
        "            d_out = cfg[ \"emb_dim\"],\n",
        "            context_length = cfg[ \"context_length\"],\n",
        "            num_heads = cfg[ \"n_heads\"],\n",
        "            dropout = cfg[ \"drop_rate\"],\n",
        "            qkv_bias = cfg[ \"qkv_bias\"],\n",
        "\n",
        "        )\n",
        "        self.ff = FeedForward( cfg )\n",
        "        self.norm1 = LayerNorm( cfg[ \"emb_dim\"])\n",
        "        self.norm2 = LayerNorm( cfg[ \"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout( cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward( self , x ):\n",
        "        shortcut = x\n",
        "        x = self.norm1( x )\n",
        "        x = self.att( x )\n",
        "        x = self.drop_shortcut( x )\n",
        "        x = x + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2( x )\n",
        "        x = self.ff( x )\n",
        "        x = self.drop_shortcut( x )\n",
        "        x = x + shortcut\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YZjshBf0xgB2"
      },
      "outputs": [],
      "source": [
        "### Now to Code the GPT Architecture\n",
        "class GPTModel( nn.Module ):\n",
        "    def __init__(self , cfg ):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding( cfg[\"vocab_size\"] , cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding( cfg[\"context_length\" ] , cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout( cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock( cfg ) for _ in range(cfg[ \"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm( cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"] , cfg[ \"vocab_size\"], bias=  False\n",
        "        )\n",
        "        self.device = None\n",
        "\n",
        "    def forward( self , in_idx ):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        if self.device :\n",
        "          in_idx = in_idx.to( self.device )\n",
        "        tok_embeds = self.tok_emb( in_idx )\n",
        "        pos_embeds = self.pos_emb( torch.arange( seq_len , device = in_idx.device ))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb( x )\n",
        "        x = self.trf_blocks( x )\n",
        "        x = self.final_norm( x )\n",
        "        logits = self.out_head( x )\n",
        "        return logits\n",
        "\n",
        "    def to(self, *args, **kwargs):\n",
        "        \"\"\"Override the to() method to store the device.\"\"\"\n",
        "        self = super().to(*args, **kwargs)\n",
        "        self.device = args[0] if args else kwargs.get('device')\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lTj407YbxgB2"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids( text , tokenizer ):\n",
        "    encoded = tokenizer.encode( text , allowed_special = {'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor( encoded ).unsqueeze( 0 )\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text( token_ids , tokenizer ):\n",
        "    flat = token_ids.squeeze( 0 )\n",
        "    return tokenizer.decode( flat.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0l6ImcRxgB3",
        "outputId": "680ce40b-fb12-4f9a-987d-b5c457a35fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version  2.18.0\n",
            "tqdm Version  4.67.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "print( \"Tensorflow version \", tf.__version__ )\n",
        "print( \"tqdm Version \", tqdm.__version__ )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "edASRY59xgB3"
      },
      "outputs": [],
      "source": [
        "def assign( left , right ):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError( f\"Shape mismatched. Left: { left.shape } , Right : { right.shape }\")\n",
        "\n",
        "    return torch.nn.Parameter( torch.tensor( right ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DueUKjlhxgB3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqDGHGYFxgB3",
        "outputId": "513873fd-cfed-4f70-f2f1-9ba3cc81d9f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 164kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.37MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 143kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [01:59<00:00, 11.8MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 13.8MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 2.40MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.58MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 1024)\n",
              "  (pos_emb): Embedding(1024, 1024)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# For this fine tuning process we will use GPT - 2 355 M parameter model\n",
        "\n",
        "from gpt_download3 import download_and_load_gpt2\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\" : 50257,\n",
        "    \"context_length\" : 1024,\n",
        "    \"drop_rate\" : 0.0,\n",
        "    \"qkv_bias\" : True\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\" : { \"emb_dim\" : 768 , \"n_layers\" : 12 , \"n_heads\" : 12 },\n",
        "    \"gpt2-medium (355M)\" : { \"emb_dim\" : 1024 , \"n_layers\" : 24 , \"n_heads\" : 16 },\n",
        "    \"gpt2-large (774M)\" : { \"emb_dim\" : 1280 , \"n_layers\" : 36 , \"n_heads\" : 20 },\n",
        "    \"gpt2-xl (1558M)\" : { \"emb_dim\" : 1600 , \"n_layers\" : 48 , \"n_heads\" : 25 },\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update( model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "\n",
        "settings , params = download_and_load_gpt2( model_size= model_size , models_dir= \"gpt2\")\n",
        "model = GPTModel( BASE_CONFIG )\n",
        "load_weights_into_gpt( model , params )\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVWHCrJQxgB4",
        "outputId": "ae826d65-9704-4b7d-d4e2-63e6eec89227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropiately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed( 123 )\n",
        "input_text = format_input( val_data[ 0 ])\n",
        "print( input_text )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7310QT7QxgB4"
      },
      "outputs": [],
      "source": [
        "def generate_text_sample( model , idx , max_new_tokens , context_size ):\n",
        "    # idx is the ( batch , n_tokens ) array of the indices in the current context\n",
        "    for _ in range( max_new_tokens ):\n",
        "        idx_cond = idx[:, -context_size: ]\n",
        "        with torch.no_grad():\n",
        "            logits = model( idx_cond )\n",
        "\n",
        "        logits = logits[ :, -1, :]\n",
        "\n",
        "        probas = torch.softmax( logits , dim = -1 )\n",
        "\n",
        "        idx_next = torch.argmax( probas , dim = -1 , keepdim = True )\n",
        "\n",
        "        idx = torch.cat((idx , idx_next ), dim = 1 )\n",
        "\n",
        "    return idx\n",
        "\n",
        "def generate( model , idx , max_new_tokens , context_size , temperarture = 0.0 , top_k = None , eos_id = None ):\n",
        "\n",
        "    idx = idx.to( model.device)\n",
        "    for _ in range( max_new_tokens ):\n",
        "        idx_cond = idx[ :, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model( idx_cond )\n",
        "        logits = logits[ :, -1, : ]\n",
        "\n",
        "        if top_k is not None :\n",
        "            # Keep only top k values\n",
        "            top_logits, _ = torch.topk( logits , top_k )\n",
        "            min_val = top_logits[ : ,-1 ]\n",
        "            logits = torch.where( logits < min_val , torch.tensor( float(\"-inf\")).to(logits.device ) , logits )\n",
        "\n",
        "        if temperarture > 0.0 :\n",
        "            logits = logits / temperarture\n",
        "\n",
        "            # Apply softmax to get the probability\n",
        "            probas = torch.softmax( logits , dim = -1 )\n",
        "\n",
        "            idx_next = torch.multinomial( probas , num_samples = 1 )\n",
        "\n",
        "        else:\n",
        "            idx_next = torch.argmax( logits , dim = -1 , keepdim = True )\n",
        "\n",
        "        if idx_next == eos_id :\n",
        "            break\n",
        "\n",
        "        idx = torch.cat(( idx , idx_next ) , dim = 1 )\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0F9tYOKGxgB4"
      },
      "outputs": [],
      "source": [
        "token_ids = generate(\n",
        "    model = model ,\n",
        "    idx = text_to_token_ids( input_text , tokenizer ) ,\n",
        "    max_new_tokens = 35 ,\n",
        "    context_size = BASE_CONFIG[\"context_length\"],\n",
        "    eos_id  = 50256,\n",
        "      )\n",
        "\n",
        "generated_text = token_ids_to_text( token_ids , tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y86Lq-8wxgCG",
        "outputId": "d09d3336-4468-4862-b17a-303b2d725f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Response:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Convert the active sentence to passive: 'The chef cooks the\n"
          ]
        }
      ],
      "source": [
        "response_text = generated_text[ len( input_text ):].strip()\n",
        "print( response_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inG6w5F3xgCG"
      },
      "source": [
        "## Fine Tuning the LLM on the Instruction Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "k7-ixh5txgCG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calc_loss_batch( input_batch , target_batch , model , device ):\n",
        "    input_batch , target_batch = input_batch.to( device ) , target_batch.to( device )\n",
        "\n",
        "    logits = model( input_batch )\n",
        "    loss = torch.nn.functional.cross_entropy( logits.flatten( 0 , 1 ) , target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader( data_loader , model , device , num_batches = None ):\n",
        "    total_loss = 0\n",
        "\n",
        "    if( len( data_loader ) == 0 ):\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None :\n",
        "        num_batches = len( data_loader )\n",
        "\n",
        "    else :\n",
        "        num_batches = min( num_batches , len( data_loader ))\n",
        "\n",
        "    for i , ( input_batch , target_batch ) in enumerate( data_loader ):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch( input_batch , target_batch , model , device )\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        else :\n",
        "            break\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def evaluate_model( model , train_loader , val_loader , device , eval_iter ):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader( train_loader , model , device , num_batches= eval_iter )\n",
        "        val_loss = calc_loss_loader( val_loader , model , device , num_batches= eval_iter )\n",
        "    model.train()\n",
        "    return train_loss , val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NiJb3dXgxgCG"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample( model , tokenizer , device , start_context ):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[ 0 ]\n",
        "    encoded = text_to_token_ids( start_context , tokenizer ).to( device )\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_sample(\n",
        "            model = model ,\n",
        "            idx = encoded ,\n",
        "            max_new_tokens= 50 ,\n",
        "            context_size= context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text( token_ids , tokenizer )\n",
        "    print( decoded_text.replace(\"\\n\" , \" \")) #compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GEUlYno0xgCG"
      },
      "outputs": [],
      "source": [
        "# The preatrained model is able to proecess and give correct output of the intsruction provided by us\n",
        "\n",
        "def train_model_simple( model , train_loader , val_loader , optimizer , device , num_epochs , eval_freq , eval_iter , start_context , tokenizer ):\n",
        "\n",
        "    train_losses , val_losses , track_tokens_seen = [] , [] , []\n",
        "    tokens_seen , global_step = 0 , -1\n",
        "\n",
        "    #Main Training Loops\n",
        "    for epoch in range( num_epochs ):\n",
        "        model.train() # Setting the model in training mode\n",
        "\n",
        "        for input_batch , target_batch in train_loader :\n",
        "            optimizer.zero_grad()\n",
        "            loss = calc_loss_batch( input_batch , target_batch , model , device )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            #optional evaluation step\n",
        "            if global_step % eval_freq == 0 :\n",
        "                train_loss , val_loss = evaluate_model(\n",
        "                    model , train_loader , val_loader , device , eval_iter\n",
        "                )\n",
        "                train_losses.append( train_loss )\n",
        "                val_losses.append( val_loss )\n",
        "                track_tokens_seen.append( tokens_seen )\n",
        "                print( f\"Ep { epoch + 1 } ( Step { global_step:06d}):\"\n",
        "                       f\"Train Loss { train_loss:.3f} , Val Loss { val_loss:.3f}\")\n",
        "\n",
        "        #Print the Sample of text generated\n",
        "        generate_and_print_sample(\n",
        "            model , tokenizer , device , start_context\n",
        "        )\n",
        "\n",
        "    return train_losses , val_losses , track_tokens_seen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnkbEokrxgCG",
        "outputId": "3249ba94-3bea-4d52-c0db-1938c426bee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss :  3.893436241149902\n",
            "Validation Loss :  3.827153968811035\n"
          ]
        }
      ],
      "source": [
        "model.to( device )\n",
        "\n",
        "torch.manual_seed( 123 )\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader( train_loader , model , device , num_batches= 5 )\n",
        "    val_loss = calc_loss_loader( val_loader , model , device , num_batches= 5 )\n",
        "\n",
        "print( \"Training Loss : \" , train_loss )\n",
        "print( \"Validation Loss : \", val_loss )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMc2jx6cxgCG",
        "outputId": "4ae92a76-c674-455e-dd78-102953972f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 ( Step 000000):Train Loss 2.645 , Val Loss 2.632\n",
            "Ep 1 ( Step 000005):Train Loss 1.147 , Val Loss 1.079\n",
            "Ep 1 ( Step 000010):Train Loss 0.842 , Val Loss 0.915\n",
            "Ep 1 ( Step 000015):Train Loss 0.822 , Val Loss 0.879\n",
            "Ep 1 ( Step 000020):Train Loss 0.748 , Val Loss 0.851\n",
            "Ep 1 ( Step 000025):Train Loss 0.724 , Val Loss 0.824\n",
            "Ep 1 ( Step 000030):Train Loss 0.764 , Val Loss 0.799\n",
            "Ep 1 ( Step 000035):Train Loss 0.694 , Val Loss 0.781\n",
            "Ep 1 ( Step 000040):Train Loss 0.648 , Val Loss 0.777\n",
            "Ep 1 ( Step 000045):Train Loss 0.611 , Val Loss 0.763\n",
            "Ep 1 ( Step 000050):Train Loss 0.644 , Val Loss 0.758\n",
            "Ep 1 ( Step 000055):Train Loss 0.737 , Val Loss 0.739\n",
            "Ep 1 ( Step 000060):Train Loss 0.699 , Val Loss 0.721\n",
            "Ep 1 ( Step 000065):Train Loss 0.634 , Val Loss 0.713\n",
            "Ep 1 ( Step 000070):Train Loss 0.516 , Val Loss 0.707\n",
            "Ep 1 ( Step 000075):Train Loss 0.551 , Val Loss 0.707\n",
            "Ep 1 ( Step 000080):Train Loss 0.585 , Val Loss 0.701\n",
            "Ep 1 ( Step 000085):Train Loss 0.494 , Val Loss 0.686\n",
            "Ep 1 ( Step 000090):Train Loss 0.548 , Val Loss 0.669\n",
            "Ep 1 ( Step 000095):Train Loss 0.482 , Val Loss 0.659\n",
            "Ep 1 ( Step 000100):Train Loss 0.484 , Val Loss 0.654\n",
            "Ep 1 ( Step 000105):Train Loss 0.550 , Val Loss 0.650\n",
            "Ep 1 ( Step 000110):Train Loss 0.539 , Val Loss 0.646\n",
            "Ep 1 ( Step 000115):Train Loss 0.495 , Val Loss 0.642\n",
            "Below is an instruction that describes a task. Write a response that appropiately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropiately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
            "Training Completed in 1.69 minutes\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed( 123 )\n",
        "\n",
        "optimizer = torch.optim.AdamW( model.parameters() , lr = 0.00005 , weight_decay= 0.1 )\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "train_losses , val_losses , tokens_seen  = train_model_simple(\n",
        "    model , train_loader , val_loader , optimizer , device ,\n",
        "    num_epochs= num_epochs , eval_freq= 5 , eval_iter= 5 ,\n",
        "    start_context= format_input( val_data[ 0 ]) , tokenizer= tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "execution_time_minutes = ( end_time - start_time ) / 60\n",
        "\n",
        "print( f\"Training Completed in {execution_time_minutes:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nVC8XhYaxgCG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "def plot_losses( epochs_seen , token_seen , train_losses , val_losses ):\n",
        "    fig , ax1 = plt.subplots( figsize = ( 5 , 3 ))\n",
        "\n",
        "    #plot Training and Validation losses against the epochs\n",
        "    ax1.plot( epochs_seen , train_losses , label = \"Training Loss\")\n",
        "    ax1.plot( epochs_seen , val_losses , linestyle = \"-.\" ,  label = \"Validation Loss\" )\n",
        "\n",
        "    ax1.set_xlabel( \"Epochs\")\n",
        "    ax1.set_ylabel( \"Loss\" )\n",
        "    ax1.legend( loc = \"upper right\")\n",
        "    ax1.xaxis.set_major_locator( MaxNLocator( integer = True ))\n",
        "\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot( token_seen , train_losses , alpha = 0 )\n",
        "    ax2.set_xlabel(\"Token_seen \")\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"loss-plot-Instruction-FineTune.pdf\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace( 0 , num_epochs , len( train_losses ))\n",
        "plot_losses( epochs_tensor , tokens_seen , train_losses , val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "vJi9Gj4K2ZcE",
        "outputId": "8166c001-27cf-49b5-ffe9-bdf880583624"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Line plot actually coverges from value of loss = 2.3 to 0.3\n",
        "# But I have run this code twice so the updatation of weights has been done twice so the values start converges from\n",
        "#0.7 something\n"
      ],
      "metadata": {
        "id": "9FWGn_oB2mPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can increase num of epochs to increase the accuracy of the model more\n"
      ],
      "metadata": {
        "id": "FXj9nt3N21wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Model weights so that I can use in my local machine"
      ],
      "metadata": {
        "id": "_L3YBCFK9000"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'fine_tuned_model_weights.pth')\n"
      ],
      "metadata": {
        "id": "47cBcnp-97B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting and Saving the Responses"
      ],
      "metadata": {
        "id": "bnIUA2gd32gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to( device )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u4A2Fd97mvw",
        "outputId": "e1f09651-4e43-4567-9139-67ce700a5fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 1024)\n",
              "  (pos_emb): Embedding(1024, 1024)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets just first check the ouput generated by llm for 3 test dataset\n",
        "print( device )\n",
        "for entry in test_data[ : 3 ]:\n",
        "  input_text = format_input( entry )\n",
        "\n",
        "\n",
        "  token_ids = generate(\n",
        "      model = model ,\n",
        "      idx = text_to_token_ids( input_text , tokenizer ).to( device ) ,\n",
        "      max_new_tokens = 256 ,\n",
        "      context_size = BASE_CONFIG[\"context_length\"],\n",
        "      eos_id= 50256\n",
        "  )\n",
        "\n",
        "  generated_text = token_ids_to_text( token_ids , tokenizer )\n",
        "  response_text = (\n",
        "      generated_text[ len( input_text ):].\n",
        "      replace( \"### Response:\" , \"\")\n",
        "      .strip()\n",
        "  )\n",
        "\n",
        "  print( input_text )\n",
        "  print( f\"\\n Correct Response: \\n >>{ entry['output']}\")\n",
        "  print( f\"\\n Model Response: \\n >>{ response_text.strip()}\")\n",
        "  print( \"-------------------------------------------------\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HaGYbyC35pb",
        "outputId": "a8abb617-e624-42d5-865a-1b8ae92fb17d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Below is an instruction that describes a task. Write a response that appropiately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            " Correct Response: \n",
            " >>The car is as fast as lightning.\n",
            "\n",
            " Model Response: \n",
            " >>The car is very fast.\n",
            "-------------------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropiately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            " Correct Response: \n",
            " >>The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            " Model Response: \n",
            " >>A thunderstorm is a type of cloud that typically occurs in the atmosphere.\n",
            "-------------------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropiately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            " Correct Response: \n",
            " >>Jane Austen.\n",
            "\n",
            " Model Response: \n",
            " >>The author of 'Pride and Prejudice' is Jane Austen.\n",
            "-------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 ways to evaluate the chatbots based on llm\n",
        "\n",
        "# We will use another LLM for evaluating the results between real and generated answer bu the Finetuned LLM\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i , entry in tqdm( enumerate( test_data ) , total= len( test_data )):\n",
        "    input_text = format_input( entry )\n",
        "\n",
        "    token_ids = generate(\n",
        "        model = model ,\n",
        "        idx = text_to_token_ids( input_text , tokenizer ).to( device ) ,\n",
        "        max_new_tokens = 256 ,\n",
        "        context_size = BASE_CONFIG[\"context_length\"],\n",
        "        eos_id= 50256\n",
        "    )\n",
        "\n",
        "    generated_text = token_ids_to_text( token_ids , tokenizer )\n",
        "    response_text = (\n",
        "        generated_text[ len( input_text ):].\n",
        "        replace( \"### Response:\" , \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    test_data[ i ][ \"model_output\" ] = response_text\n",
        "\n",
        "with open(\"instruction-data-with-response.json\" , \"w\") as file:\n",
        "  json.dump( test_data , file , indent = 4 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ijiECCbFy-a",
        "outputId": "2da4807a-b77d-448f-fd5f-a4918379cad8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [01:21<00:00,  1.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( test_data[ 0 ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk8IqcEgIpe7",
        "outputId": "39cb7f88-3abd-4b23-e13c-f10e7593a5ab"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_output': 'The car is very fast.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Fine - Tune LLM using OLLAMA"
      ],
      "metadata": {
        "id": "ioIZT551JitK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload and install LLaMa 3 b in personal computer using OLLama\n",
        "# And run in terminal\n",
        "\n",
        "\n",
        "!pip install transformers accelerate torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "def generate_text(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_length=200)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generate_text(\"Hello, how are you?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qFjJZaixJDP3",
        "outputId": "4d894121-fcce-45b5-e216-69b38b85f991"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m874.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "4bcf7ade168c4219be2ff065b2959344"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "meta-llama/Llama-3B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3B/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67d406f4-7c6d6fc470cc51cd12285243;e6d61387-1e8c-4287-870c-c816628adf71)\n\nRepository Not Found for url: https://huggingface.co/meta-llama/Llama-3B/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-8ae2c9330410>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Llama-3B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         ) from e\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: meta-llama/Llama-3B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in test_data[:3]:\n",
        "\n",
        "  prompt = (\n",
        "      f\"Given the input `{ format_input( entry )}`\"\n",
        "      f\"and correct output `{ entry['output']}`,\"\n",
        "      f\"score the model reponse  `{ entry['model_response']}`\"\n",
        "      f\"on scale of 0 to 100, where 100 is the best score.\"\n",
        "  )\n",
        "\n",
        "  print(\"\\nDataset response:\")\n",
        "  print(\">>\", entry['output '])\n",
        "  print(\"\\nModel response:\")\n",
        "  print(\">>\", entry[\"model_response\"])\n",
        "  print (\"\\nScore:\")\n",
        "  print(\">>\", generate_text(prompt) )\n",
        "  print (\"\\n-------------------------\" )"
      ],
      "metadata": {
        "id": "2iJfJvRDLnNY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}