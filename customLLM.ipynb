{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  This is the Entire PipeLine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 2159, 220]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding( \"gpt2\" )\n",
    "text = ( \"Hello World \")\n",
    "integers = tokenizer.encode( text )\n",
    "print ( integers )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 256,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalisation and Feed Forward Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm( nn.Module ):\n",
    "    def __init__( self , emb_dim ):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter( torch.ones( emb_dim ))\n",
    "        self.shift = nn.Parameter( torch.zeros( emb_dim ))\n",
    "\n",
    "\n",
    "    def forward( self , x ):\n",
    "        mean = x.mean( dim = -1 , keepdim = True )\n",
    "        var = x.var( dim = -1 , keepdim = True , unbiased = True )\n",
    "        norm_x = ( x - mean ) / torch.sqrt( var + self.eps )\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "\n",
    "class GELU( nn.Module ):\n",
    "    def __init__( self  ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward( self , x ):\n",
    "        return 0.5 * x * ( 1 + torch.tanh( \n",
    "            torch.sqrt( torch.tensor( 2.0 / torch.pi )) * \n",
    "            ( x + 0.044715 * torch.pow( x , 3 )) \n",
    "        ))\n",
    "\n",
    "class FeedForward( nn.Module ):\n",
    "    def __init__( self , cfg ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear( cfg[\"emb_dim\" ] , 4 * cfg[\"emb_dim\" ] ),\n",
    "            GELU(),\n",
    "            nn.Linear( 4 * cfg[ \"emb_dim\"] , cfg[ \"emb_dim\" ]),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward( self , x ):\n",
    "        return self.layers( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MutiHead Attention Mech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock( nn.Module ):\n",
    "    def __init__( self , cfg ):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention( \n",
    "            d_in = cfg[ \"emb_dim\"],\n",
    "            d_out = cfg[ \"emb_dim\"],\n",
    "            context_length = cfg[ \"context_length\"],\n",
    "            num_heads = cfg[ \"n_heads\"],\n",
    "            dropout = cfg[ \"drop_rate\"],\n",
    "            qkv_bias = cfg[ \"qkv_bias\"],\n",
    "\n",
    "        )\n",
    "        self.ff = FeedForward( cfg )\n",
    "        self.norm1 = LayerNorm( cfg[ \"emb_dim\"])\n",
    "        self.norm2 = LayerNorm( cfg[ \"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout( cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward( self , x ):\n",
    "        shortcut = x\n",
    "        x = self.norm1( x )\n",
    "        x = self.att( x )\n",
    "        x = self.drop_shortcut( x )\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.ff( x )\n",
    "        x = self.drop_shortcut( x )\n",
    "        x = x + shortcut\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to Code the GPT Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel( nn.Module ):\n",
    "    def __init__(self , cfg ):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding( cfg[\"vocab_size\"] , cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding( cfg[\"context_length\" ] , cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout( cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential( \n",
    "            *[TransformerBlock( cfg ) for _ in range(cfg[ \"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm( cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"] , cfg[ \"vocab_size\"], bias=  False\n",
    "        )\n",
    "\n",
    "    def forward( self , in_idx ):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb( in_idx )\n",
    "        pos_embeds = self.pos_emb( torch.arange( seq_len , device = in_idx.device ))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb( x )\n",
    "        x = self.trf_blocks( x )\n",
    "        x = self.final_norm( x )\n",
    "        logits = self.out_head( x )\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Input batch : \n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output : \n",
      " torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.7930e-01,  2.8437e-01, -7.6031e-01,  ..., -4.8328e-01,\n",
      "          -4.2459e-01, -1.7183e-01],\n",
      "         [-6.2540e-01, -3.7472e-01, -9.6911e-01,  ...,  1.9217e-01,\n",
      "          -1.3227e+00, -2.7610e-01],\n",
      "         [ 5.1774e-01,  1.3858e-01,  2.4931e-01,  ...,  3.5055e-01,\n",
      "          -7.7138e-02, -7.9681e-02],\n",
      "         [-2.5624e-01, -6.9668e-01, -9.9379e-01,  ..., -4.4323e-02,\n",
      "           6.1901e-02,  1.3455e-01]],\n",
      "\n",
      "        [[-2.2345e-01,  1.1610e-01, -9.9719e-01,  ..., -1.5698e-01,\n",
      "          -4.4743e-01, -2.8685e-02],\n",
      "         [-8.7201e-01, -3.9409e-01, -1.1089e+00,  ...,  3.3024e-01,\n",
      "          -9.2300e-02, -9.1573e-05],\n",
      "         [ 4.5925e-01, -1.4261e-01, -1.2172e-01,  ...,  2.7475e-01,\n",
      "           5.8875e-02, -9.0158e-02],\n",
      "         [-6.2152e-01, -4.4847e-01, -4.7587e-01,  ..., -3.6475e-01,\n",
      "           3.4392e-01, -3.8138e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "torch.manual_seed( 123 )\n",
    "model = GPTModel( GPT_CONFIG_124M )\n",
    "out = model( batch )\n",
    "print( \"Input batch : \\n\" , batch )\n",
    "print( \"Output : \\n\" , out.shape )\n",
    "print( out )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the total Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Parameters :  162,419,712\n"
     ]
    }
   ],
   "source": [
    "total_params = sum( p.numel() for p in model.parameters())\n",
    "print(f\"Total number of Parameters :  { total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding layer shape:  torch.Size([50257, 768])\n",
      "Output layer shape:  torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print( \"Token Embedding layer shape: \" , model.tok_emb.weight.shape )\n",
    "print( \"Output layer shape: \" , model.out_head.weight.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tainable parameters are :123,822,336\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum( p.numel() for p in model.out_head.parameters())\n",
    "print( f\"Number of Tainable parameters are :{total_params_gpt2 :,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_sample( model , idx , max_new_tokens , context_size ):\n",
    "    # idx is the ( batch , n_tokens ) array of the indices in the current context\n",
    "    for _ in range( max_new_tokens ):\n",
    "        idx_cond = idx[:, -context_size: ]\n",
    "        with torch.no_grad():\n",
    "            logits = model( idx_cond )\n",
    "\n",
    "        logits = logits[ :, -1, :]\n",
    "\n",
    "        probas = torch.softmax( logits , dim = -1 )\n",
    "    \n",
    "        idx_next = torch.argmax( probas , dim = -1 , keepdim = True )\n",
    "\n",
    "        idx = torch.cat((idx , idx_next ), dim = 1 )\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded :  [15496, 11, 314, 716]\n",
      "Encoded_tensor.shape :  torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_content = \"Hello, I am\"\n",
    "encoded = tokenizer.encode( start_content )\n",
    "print( \"encoded : \" , encoded )\n",
    "encoded_tensor = torch.tensor( encoded ).unsqueeze( 0 )\n",
    "print( \"Encoded_tensor.shape : \", encoded_tensor.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :  tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991]])\n",
      "Output Length :  10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_sample( \n",
    "    model= model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens= 6,\n",
    "    context_size= GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output : \", out )\n",
    "print(\"Output Length : \", len(out[ 0 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Laur inhab DistrinetalkQueue\n",
      "These are random text , the Parameter of the GPT is now set to be learn during the training\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode( out.squeeze(0).tolist())\n",
    "print( decoded_text )\n",
    "print( \"These are random text , the Parameter of the GPT is now set to be learn during the training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text :\n",
      "  Every effort moves you,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids( text , tokenizer ):\n",
    "    encoded = tokenizer.encode( text , allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor( encoded ).unsqueeze( 0 )\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text( token_ids , tokenizer ):\n",
    "    flat = token_ids.squeeze( 0 )\n",
    "    return tokenizer.decode( flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_sample(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids( start_context , tokenizer ),\n",
    "    max_new_tokens= 10,\n",
    "    context_size= GPT_CONFIG_124M[\"context_length\"] \n",
    ")\n",
    "\n",
    "print( \"output text :\\n \" , token_ids_to_text( token_ids , tokenizer ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[ 16833 , 3626 , 6100 ],\n",
    "                       [ 40 , 1107 , 588 ]])\n",
    "\n",
    "targets = torch.tensor([[ 3626, 6100, 345 ],\n",
    "                        [ 1107, 588,  11311 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model( inputs )\n",
    "\n",
    "probas = torch.softmax( logits, dim = -1 )\n",
    "print( probas.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ids : \n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax( probas , dim = -1 , keepdim = True )\n",
    "print(\"token ids : \\n\" , token_ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target batch 1:  effort moves you\n",
      "output batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# Lets decode it\n",
    "print( f\"Target batch 1: { token_ids_to_text( targets[ 0 ] , tokenizer )}\")\n",
    "print( f\"output batch 1: { token_ids_to_text( token_ids[ 0 ].flatten() , tokenizer )}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corss Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 :  tensor([7.4514e-05, 3.1054e-05, 1.1567e-05])\n",
      "Text 2 :  tensor([1.0343e-05, 5.6737e-05, 4.7620e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0 \n",
    "target_probas_1 = probas[ text_idx , [ 0 , 1 , 2 ] , targets[ text_idx ]]\n",
    "print( \"Text 1 : \", target_probas_1 )\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[ text_idx , [ 0 , 1 , 2 ] , targets[ text_idx ]]\n",
    "print( \"Text 2 : \" , target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5045, -10.3798, -11.3674, -11.4792,  -9.7771, -12.2549])\n"
     ]
    }
   ],
   "source": [
    "# Taking the Log\n",
    "log_probas = torch.log( torch.cat(( target_probas_1 , target_probas_2)))\n",
    "print( log_probas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7938)\n"
     ]
    }
   ],
   "source": [
    "# Takin  the mean\n",
    "avg_log_probas = torch.mean( log_probas )\n",
    "print( avg_log_probas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7938)\n"
     ]
    }
   ],
   "source": [
    "# (-)ve of the log likelihood\n",
    "neg_avg_log_probas = -1 * avg_log_probas\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits Shape :  torch.Size([2, 3, 50257])\n",
      "Target Shape :  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Alternate  way of doing it \n",
    "print( \"Logits Shape : \" , logits.shape )\n",
    "print( \"Target Shape : \", targets.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Logits :  torch.Size([6, 50257])\n",
      "Flattened Target :  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Can be done in few line of codes\n",
    "logits_flat = logits.flatten( 0 , 1 )\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print( \"Flattened Logits : \" , logits_flat.shape )\n",
    "print( \"Flattened Target : \", targets_flat.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7938)\n"
     ]
    }
   ],
   "source": [
    "# Loss calculation through one line of code\n",
    "loss = torch.nn.functional.cross_entropy( logits_flat , targets_flat )\n",
    "print( loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48717.6875)\n"
     ]
    }
   ],
   "source": [
    "perplexcity = torch.exp( loss )\n",
    "print( perplexcity )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on Toy Data Set - The Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# file_path = \"the-verdict.txt\"\n",
    "# url = \"https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "# if not os.path.exists( file_path ):\n",
    "#     with urllib.request.urlopen( url ) as response :\n",
    "#         text_data = response.read().decode('utf-8')\n",
    "\n",
    "#     with open( file_path , \"w\" , encoding='utf-8' ) as file:\n",
    "#         file.write( text)\n",
    "with open(\"/Users/adityaastronomy/Downloads/the_Verdict.txt\", \"r\", encoding=\"utf-8\") as file :\n",
    "    text_data = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Verdict\n",
      "Edith Wharton\n",
      "\n",
      "1908\n",
      "\n",
      "Exported from Wikisource on May 20, 2024\n",
      "\n",
      "1\n",
      "\n",
      "\fI HAD always thought\n"
     ]
    }
   ],
   "source": [
    "# print firts 100 words\n",
    "print( text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "Blurpeace\n",
      "Dschwen\n",
      "Boris23\n",
      "KABALINI\n",
      "Bromskloss\n",
      "Tene~commonswiki\n",
      "AzaToth\n",
      "Bender235\n",
      "Patr√≠ciaR\n",
      "\n",
      "22\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "# Print the last 1000 characters\n",
    "print( text_data[-99 :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters :  21871\n",
      "Tokens :  5898\n"
     ]
    }
   ],
   "source": [
    "total_characters = len( text_data )\n",
    "total_token = len( tokenizer.encode( text_data ))\n",
    "print( \"Characters : \", total_characters )\n",
    "print( \"Tokens : \", total_token )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spilting the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1( Dataset ):\n",
    "    def __init__( self , txt, tokenizer , max_length , stride ):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        # Tokenise the entire text \n",
    "        token_ids = tokenizer.encode( txt , allowed_special= {\"<|endoftext|>\"})\n",
    "        # use of sliding window to chunk the book into overlapping sequene of max_length \n",
    "\n",
    "        for i in range( 0 , len( token_ids ) - max_length , stride ):\n",
    "            input_chunk = token_ids[ i:i + max_length ]\n",
    "            target_chunk = token_ids[ i + 1:i + max_length + 1 ]\n",
    "            self.input_ids.append( torch.tensor( input_chunk ))\n",
    "            self.target_ids.append( torch.tensor( target_chunk ))\n",
    "    \n",
    "    def __len__( self ):\n",
    "        return len( self.input_ids )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[ index ], self.target_ids[ index ]\n",
    "    \n",
    "\n",
    "def create_dataloader_v1( txt , batch_size = 4 , max_length = 256 , stride = 126 , shuffle = True , drop_last = True , num_workers = 0 ):\n",
    "    # Initialise the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding( \"gpt2\")\n",
    "    # create dataset \n",
    "    dataset = GPTDatasetV1( txt , tokenizer , max_length , stride )\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset , \n",
    "        batch_size= batch_size ,\n",
    "        shuffle= shuffle,\n",
    "        drop_last= drop_last,\n",
    "        num_workers= num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation Ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int( train_ratio * len( text_data ))\n",
    "train_data = text_data[ : split_idx ]\n",
    "val_data = text_data[ split_idx : ]\n",
    "\n",
    "torch.manual_seed( 123 )\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size= 2,\n",
    "    max_length= GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride= GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last= True,\n",
    "    shuffle = True,\n",
    "    num_workers= 0 # this is used for the parallel processing\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size= 2,\n",
    "    max_length= GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride= GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last= False,\n",
    "    shuffle = False,\n",
    "    num_workers= 0 # this is used for the parallel processing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5308.2\n",
      "589.7999999999998\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "if total_token * ( train_ratio ) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print( \"Not Enough token for the training loader \")\n",
    "\n",
    "if( total_token * ( 1 - train_ratio ) < GPT_CONFIG_124M[\"context_length\"]):\n",
    "    print( \"Not Enough token for the Validation Loader \")\n",
    "\n",
    "print( total_token * ( train_ratio ))\n",
    "print( total_token * ( 1 - train_ratio ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "validation Loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print( \"Train Loader: \")\n",
    "for x, y in train_loader:\n",
    "    print( x.shape , y.shape )\n",
    "\n",
    "\n",
    "print( \"validation Loader: \")\n",
    "for x, y in val_loader:\n",
    "    print( x.shape , y.shape )\n",
    "\n",
    "print( len( train_loader ))\n",
    "print( len( val_loader ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed( 123 )\n",
    "model = GPTModel( GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the loss of the output with the targets \n",
    "\n",
    "def calc_loss_batch( input_batch , target_batch , model , device ):\n",
    "    input_batch , target_batch = input_batch.to( device ), target_batch.to( device )\n",
    "    logits = model( input_batch )\n",
    "    loss = torch.nn.functional.cross_entropy( logits.flatten( 0 , 1 ) , target_batch.flatten())\n",
    "    return loss\n",
    " \n",
    "def cal_losss_loader( data_loader, model, device, num_batches = None ):\n",
    "    total_loss = 0.\n",
    "    if( len(data_loader)) == 0 :\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len( data_loader )\n",
    "\n",
    "    else :\n",
    "        num_batches = min( num_batches , len( data_loader ))\n",
    "    \n",
    "    for i, ( input_batch , target_batch ) in enumerate( data_loader ):\n",
    "        if i < num_batches :\n",
    "            loss = calc_loss_batch( input_batch , target_batch , model , device )\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    return total_loss/ num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss :  10.978403282165527\n",
      "Validation Loss :  11.004152297973633\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(  \"cpu\")\n",
    "model.to( device )\n",
    "\n",
    "torch.manual_seed( 123 )\n",
    "with torch.no_grad():\n",
    "    train_loss = cal_losss_loader( train_loader , model , device )\n",
    "    val_loss = cal_losss_loader( val_loader , model , device )\n",
    "\n",
    "print(\"Training Loss : \" , train_loss )\n",
    "print(\"Validation Loss : \" , val_loss )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sample( model , train_loader , val_loader , optimizer, device , num_epochs, eval_freq , eval_iter, start_context, tokenizer ):\n",
    "    # Initialise the list to track losses and token seen\n",
    "    train_losses , val_losses , track_token_seen = [] , [] , []\n",
    "    token_seen , global_step = 0, -1\n",
    "\n",
    "    # Main triaining loop\n",
    "    \n",
    "    for epoch in range( num_epochs ):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch , traget_batch in train_loader : \n",
    "            optimizer.zero_grad() # Reset the Loss Gradient from the previous batch\n",
    "            loss = calc_loss_batch( input_batch , traget_batch , model , device )\n",
    "            loss.backward()     # Calculate the loss gradient \n",
    "            optimizer.step()    #update the model weights \n",
    "            token_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            #Optinal Evalution Step\n",
    "            if global_step % eval_freq == 0 :\n",
    "                train_loss , val_loss = evaluated_model(\n",
    "                    model, train_loader, val_loader, device , eval_iter \n",
    "                )\n",
    "                train_losses.append( train_loss )\n",
    "                val_losses.append( val_loss )\n",
    "                track_token_seen.append( token_seen )\n",
    "                print( f\"Ep {epoch + 1 } ( Step { global_step :06d}) :  \"\n",
    "                        f\" Train Loss { train_loss:.3f}, Val Loss { val_loss :.3f}\" )\n",
    "        # Print the sample text after each epoch \n",
    "\n",
    "        generate_and_print_sample( \n",
    "            model , tokenizer , device , start_context \n",
    "        )    \n",
    "\n",
    "    return train_losses , val_losses , track_token_seen \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluated_model( model , train_loader , val_loader , device , eval_iter ):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = cal_losss_loader( train_loader , model ,device , num_batches = eval_iter )\n",
    "        val_loss = cal_losss_loader( val_loader , model , device , num_batches = eval_iter )\n",
    "    model.train()\n",
    "    return train_loss , val_loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample( model , tokenizer , device , start_context ):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[ 0 ]\n",
    "    encoded = text_to_token_ids( start_context , tokenizer ).to( device )\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_sample(\n",
    "            model = model ,\n",
    "            idx = encoded ,\n",
    "            max_new_tokens= 50 ,\n",
    "            context_size= context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text( token_ids , tokenizer )\n",
    "    print( decoded_text.replace(\"\\n\" , \" \")) #compact print format \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 ( Step 000000) :   Train Loss 9.859, Val Loss 9.933\n",
      "Ep 1 ( Step 000005) :   Train Loss 8.173, Val Loss 8.615\n",
      "Every effort moves you, ,,, ,,,,,, ,,,, ,,, ,,, ,,, ,, ,,,, ,,, the ,,,,,,,\n",
      "Ep 2 ( Step 000010) :   Train Loss 6.855, Val Loss 7.860\n",
      "Ep 2 ( Step 000015) :   Train Loss 6.205, Val Loss 7.748\n",
      "Every effort moves you, , , , , ,,  , ,  ,  , , , , , , , , , , , , , , \n",
      "Ep 3 ( Step 000020) :   Train Loss 6.140, Val Loss 7.885\n",
      "Ep 3 ( Step 000025) :   Train Loss 5.808, Val Loss 7.908\n",
      "Every effort moves you \" \", \" \" \", and \" \" \" \" \" \" \", the \" \" \" \" \" \" \" \" \" \",\n",
      "Ep 4 ( Step 000030) :   Train Loss 5.683, Val Loss 7.975\n",
      "Ep 4 ( Step 000035) :   Train Loss 5.492, Val Loss 7.925\n",
      "Every effort moves you, and \"I \"I \"II had \"I, and \"I \"I \"II the \"I \"I had the \"I \"I \"II \"I\n",
      "Ep 5 ( Step 000040) :   Train Loss 4.850, Val Loss 7.887\n",
      "Ep 5 ( Step 000045) :   Train Loss 4.466, Val Loss 7.830\n",
      "Every effort moves you \" \"I to my way to \"I could it. \"I \"I \" to have was not that, and it, and \"I had to have a \"I had been \"I \"\n",
      "Ep 6 ( Step 000050) :   Train Loss 3.849, Val Loss 7.662\n",
      "Ep 6 ( Step 000055) :   Train Loss 3.484, Val Loss 7.767\n",
      "Every effort moves you know; and in \"Oh, I had I could just manage to see it, I had \" to \"I had \"I had \"Oh, I had always--the \"Oh, I had \"I\n",
      "Ep 7 ( Step 000060) :   Train Loss 3.119, Val Loss 7.722\n",
      "Ep 7 ( Step 000065) :   Train Loss 2.432, Val Loss 7.761\n",
      "Every effort moves you know; and \" to my way to the \"I had been \"--and I was \" to her to have him, I had the pines, I had myself borne thither the next day. \"Oh, in\n",
      "Ep 8 ( Step 000070) :   Train Loss 1.987, Val Loss 7.767\n",
      "Ep 8 ( Step 000075) :   Train Loss 1.592, Val Loss 7.812\n",
      "Every effort moves you know; and I felt as if he had theI must really see your portrait, and I.\" She glanced out almost timorously at the terrace where her Well!--even through the prism of the--I must found the couple\n",
      "Ep 9 ( Step 000080) :   Train Loss 1.320, Val Loss 7.884\n",
      "Ep 9 ( Step 000085) :   Train Loss 0.975, Val Loss 7.939\n",
      "Every effort moves you know; and having, stay.\" Yes--I could just manage to see it--the first portrait of Jack's I had ever had to strain my eyes over! Usually they had the place of honour--say the central panel in\n",
      "Ep 10 ( Step 000090) :   Train Loss 0.765, Val Loss 7.954\n",
      "Ep 10 ( Step 000095) :   Train Loss 0.510, Val Loss 8.107\n",
      "Every effort moves you know; and having, on my way to Monte 3  \fCarlo, caught a glimpse of Jack's balustraded terraces between the pines, I had myself borne thither the next day. I found the couple\n",
      "Training Completed in 7.26 minutes \n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed( 123 )\n",
    "model = GPTModel( GPT_CONFIG_124M )\n",
    "model.to( device )\n",
    "optimizer = torch.optim.AdamW( model.parameters() , lr = 0.0004 , weight_decay = 0.1 )  # we are using AdamW optimizer \n",
    "\n",
    "num_epoches = 10\n",
    "train_losses , val_losses , token_seen = train_model_sample(\n",
    "    model , train_loader , val_loader , optimizer , device , num_epochs = num_epoches , eval_freq = 5 , eval_iter = 5 , \n",
    "    start_context= \"Every effort moves you\" , tokenizer= tokenizer \n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = ( end_time - start_time ) / 60 \n",
    "print( f\"Training Completed in { execution_time_minutes:.2f} minutes \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loss Become small but the Validation Loss become stagnent after sometime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adityaastronomy/Library/Python/3.12/lib/python/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/adityaastronomy/Library/Python/3.12/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/adityaastronomy/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses( epoches_seen , token_seen , train_losses , val_losses ):\n",
    "    fig , ax1 = plt.subplots( figsize= ( 5 , 3 ))\n",
    "\n",
    "    # Plot the training and validation losses against the epoches \n",
    "    ax1.plot( epoches_seen , train_losses , label = \"Training Losses\" )\n",
    "    ax1.plot( epoches_seen , val_losses , label = \"Validation_Losses\")\n",
    "    ax1.set_xlabel( \"Epoches\")\n",
    "    ax1.set_ylabel( \"Loss\" )\n",
    "    ax1.legend( loc = \"upper right\")\n",
    "    ax1.xaxis.set_major_locator( MaxNLocator( integer = True ))\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot( token_seen , train_losses , alpha = 0 )\n",
    "    ax2.set_xlabel( \"Token_seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace( 0 , num_epoches , len( train_losses ))\n",
    "plot_losses( epochs_tensor , token_seen , train_losses , val_losses )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies to Control the Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output test: \n",
      " Every effort moves you know; and having, on my way to Monte\n",
      "3\n",
      "\n",
      "\fCarlo, caught a glimpse of Jack's balust\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_sample(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids( \"Every effort moves you\" , tokenizer ),\n",
    "    max_new_tokens = 25,\n",
    "    context_size = GPT_CONFIG_124M[ \"context_length\" ]\n",
    "    )\n",
    "\n",
    "print( \"Output test: \\n\" , token_ids_to_text( token_ids , tokenizer ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategy 1: Temperature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the argmax with the probability distribution\n",
    "\n",
    "vocab = {\n",
    "    \"closer\" : 0,\n",
    "    \"every\" : 1,\n",
    "    \"effort\" : 2,\n",
    "    \"forward\" : 3,\n",
    "    \"inches\" : 4,\n",
    "    \"moves\" : 5,\n",
    "    \"pizza\" : 6,\n",
    "    \"towards\" : 7,\n",
    "    \"you\" : 8,\n",
    "}\n",
    "\n",
    "reverse_vocab = { v : k for k , v in vocab.items() } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose the start context is \"Enery effort moves you\" therefore the output, logits tensor is\n",
    "next_token_logits = torch.tensor(\n",
    "    [ 4.51 , 0.89 , -1.90, 6.75 , 1.63 , -1.62, -1.89 , 6.28 ,1.79 ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0907e-02, 1.6313e-03, 1.0019e-04, 5.7212e-01, 3.4190e-03, 1.3257e-04,\n",
      "        1.0120e-04, 3.5758e-01, 4.0122e-03])\n",
      "3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax( next_token_logits , dim = 0 )\n",
    "\n",
    "print( probas )\n",
    "next_token_id = torch.argmax( probas ).item()\n",
    "print( next_token_id )\n",
    "print( reverse_vocab[ next_token_id ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Now replacing the argmax with multinomial function \n",
    "torch.manual_seed( 123 )\n",
    "next_token_id = torch.multinomial( probas , num_samples= 1 ).item()\n",
    "print( reverse_vocab[ next_token_id ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x towards\n"
     ]
    }
   ],
   "source": [
    "# Running the funtions 1000 times to see the variance in the answer but most of the time it will be the \"forward\"\n",
    "def print_sample_tokens( probas ):\n",
    "    torch.manual_seed( 123 )\n",
    "    sample = [torch.multinomial( probas , num_samples = 1).item() for i in range( 1000 )]\n",
    "    sampled_ids = torch.bincount( torch.tensor( sample ))\n",
    "    for i , freq in enumerate( sampled_ids ):\n",
    "        print( f\"{freq} x {reverse_vocab[ i ]}\")\n",
    "\n",
    "print_sample_tokens( probas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the fancy concept of the Temperature\n",
    "def softmax_with_temperature( logits , temperature ):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax( scaled_logits , dim = 0 )\n",
    "\n",
    "# Temperature Values \n",
    "temperatures = [ 1 , 0.1 , 5 ]\n",
    "\n",
    "# calculate scaled probabilities \n",
    "scaled_probas = [ softmax_with_temperature( next_token_logits , T ) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "x = torch.arange( len( vocab ))\n",
    "bar_width = 0.15 \n",
    "\n",
    "fig, ax = plt.subplots( figsize = ( 5 , 3 ))\n",
    "for i , T in enumerate( temperatures ):\n",
    "    rects = ax.bar( x + i * bar_width , scaled_probas[ i ] , bar_width , label = f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel( 'Probability')\n",
    "ax.set_xticks( x )\n",
    "ax.set_xticklabels( vocab.keys() , rotation = 90 )\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8530e-10, 3.5189e-26, 2.6890e-38, 9.9099e-01, 5.7569e-23, 4.4220e-37,\n",
      "        2.9718e-38, 9.0133e-03, 2.8514e-22])\n"
     ]
    }
   ],
   "source": [
    "next_token_logits2 = next_token_logits / 0.1 \n",
    "probas2 = torch.softmax( next_token_logits2 , dim = 0 )\n",
    "\n",
    "print( probas2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategy 2: Top - K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Logits :  tensor([6.7500, 6.2800, 4.5100])\n",
      "Top Positions :  tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits , top_pos = torch.topk( next_token_logits , top_k )\n",
    "print(\"Top Logits : \", top_logits )\n",
    "print(\"Top Positions : \", top_pos )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where( \n",
    "    condition= next_token_logits < top_logits[ -1 ],\n",
    "    input = torch.tensor( float(\"-inf\")),\n",
    "    other = next_token_logits\n",
    ")\n",
    "print( new_logits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax( new_logits , dim = 0 )\n",
    "print( topk_probas )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top k + Temperature Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate( model , idx , max_new_tokens , context_size , temperarture = 0.0 , top_k = None , eos_id = None ):\n",
    "\n",
    "    for _ in range( max_new_tokens ):\n",
    "        idx_cond = idx[ :, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model( idx_cond )\n",
    "        logits = logits[ :, -1, : ]\n",
    "\n",
    "        if top_k is not None :\n",
    "            # Keep only top k values \n",
    "            top_logits, _ = torch.topk( logits , top_k )\n",
    "            min_val = top_logits[ : ,-1 ]\n",
    "            logits = torch.where( logits < min_val , torch.tensor( float(\"-inf\")).to(logits.device ) , logits )\n",
    "            \n",
    "        if temperarture > 0.0 :\n",
    "            logits = logits / temperarture \n",
    "\n",
    "            # Apply softmax to get the probability \n",
    "            probas = torch.softmax( logits , dim = -1 )\n",
    "\n",
    "            idx_next = torch.multinomial( probas , num_samples = 1 )\n",
    "\n",
    "        else:\n",
    "            idx_next = torch.argmax( logits , dim = -1 , keepdim = True )\n",
    "\n",
    "        if idx_next == eos_id :\n",
    "            break\n",
    "\n",
    "        idx = torch.cat(( idx , idx_next ) , dim = 1 )\n",
    "\n",
    "    return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text \n",
      " tensor([[ 6109,  3626,  6100,   345,   821,   553,   673,  3114,   340,   284,\n",
      "           502,   783,   339,  8288,  4964,   502,   466, 22486,    11]])\n",
      "Every effort moves you're,\" she looked it to me now he liked watching me do balancing,\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed( 123 )\n",
    "\n",
    "token_ids = generate(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids( \"Every effort moves you\" , tokenizer ),\n",
    "    max_new_tokens = 15 ,\n",
    "    context_size= GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k= 25 , \n",
    "    temperarture= 2\n",
    ")\n",
    "\n",
    "print(\"Output text \\n\" , token_ids )\n",
    "print( token_ids_to_text( token_ids, tokenizer ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving the model weight in Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "torch.save( model.state_dict() , \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/j8l4b0cj5zqdl0273gsv8jgh0000gn/T/ipykernel_41380/3255646474.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict( torch.load( \"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets now check by loading and seeing its parameter\n",
    "model = GPTModel( GPT_CONFIG_124M )\n",
    "model.load_state_dict( torch.load( \"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW( model.parameters() , lr = 0.0004 , weight_decay= 0.1 )\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "    \"model_state_dict\" : model.state_dict(),\n",
    "    \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/j8l4b0cj5zqdl0273gsv8jgh0000gn/T/ipykernel_41380/3312451483.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoints = torch.load( \"model_and_optimizer.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints = torch.load( \"model_and_optimizer.pth\")\n",
    "model = GPTModel( GPT_CONFIG_124M )\n",
    "model.load_state_dict( checkpoints[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW( model.parameters() , lr = 5e-4 , weight_decay= 0.1)\n",
    "optimizer.load_state_dict( checkpoints[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained weights from Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (25.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/adityaastronomy/Library/Python/3.12/lib/python/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/adityaastronomy/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/adityaastronomy/Library/Python/3.12/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: 2.15.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install tqdm\n",
    "%pip install tensorflow>=2.15.0 tqdm>=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version  2.18.0\n",
      "tqdm Version  4.67.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "print( \"Tensorflow version \", tf.__version__ )\n",
    "print( \"tqdm Version \", tqdm.__version__ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download3 import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: /Users/adityaastronomy/Desktop/LLM/124M/checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: /Users/adityaastronomy/Desktop/LLM/124M/encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: /Users/adityaastronomy/Desktop/LLM/124M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: /Users/adityaastronomy/Desktop/LLM/124M/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: /Users/adityaastronomy/Desktop/LLM/124M/model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: /Users/adityaastronomy/Desktop/LLM/124M/model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: /Users/adityaastronomy/Desktop/LLM/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, param =  download_and_load_gpt2( model_size= \"124M\" , models_dir= \"/Users/adityaastronomy/Desktop/LLM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting :  {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys :  dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print( \"Setting : \", settings)\n",
    "print( 'Parameter dictionary keys : ' , param.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token Embeddings weight tensor dimensions :  (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "# Lets just check the value of the param[ wte ]\n",
    "print(param['wte'])\n",
    "print(\"Token Embeddings weight tensor dimensions : \" , param[ \"wte\" ].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\" : { \"emb_dim\" : 768 , \"n_layers\" : 12 , \"n_heads\" : 12 },\n",
    "    \"gpt2-medium (355M)\" : { \"emb_dim\" : 1024 , \"n_layers\" : 24 , \"n_heads\" : 16 }, \n",
    "    \"gpt2-large (774M)\" : { \"emb_dim\" : 1280 , \"n_layers\" : 36 , \"n_heads\" : 20 }, \n",
    "    \"gpt2-xl (1558M)\" : { \"emb_dim\" : 1600 , \"n_layers\" : 48 , \"n_heads\" : 25 }, \n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update( model_configs[ model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': True}\n",
      "{'vocab_size': 50257, 'context_length': 256, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
     ]
    }
   ],
   "source": [
    "NEW_CONFIG.update( {\"context_length\" : 1024 , \"qkv_bias\" : True })\n",
    "gpt = GPTModel( NEW_CONFIG )\n",
    "gpt.eval()\n",
    "print( NEW_CONFIG)\n",
    "print( GPT_CONFIG_124M )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign( left , right ):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError( f\"Shape mismatched. Left: { left.shape } , Right : { right.shape }\")\n",
    "    \n",
    "    return torch.nn.Parameter( torch.tensor( right ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now link the GPT Model class with the downloaded weights\n",
    "import numpy as np\n",
    "\n",
    "def load_weight_into_gpt( gpt , params ):\n",
    "    gpt.pos_emb.weight = assign( gpt.pos_emb.weight , params[ 'wpe'])\n",
    "    gpt.tok_emb.weight = assign( gpt.tok_emb.weight , params[ 'wte'])\n",
    "\n",
    "    for b in range( len( params[ 'blocks'])):\n",
    "        # weight part \n",
    "        q_w, k_w , v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"] , 3 , axis = -1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign( gpt.trf_blocks[b].att.W_query.weight , q_w.T )\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(  gpt.trf_blocks[b].att.W_key.weight , k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign( gpt.trf_blocks[b].att.W_value.weight , v_w.T )\n",
    "\n",
    "        # bias part \n",
    "        q_b , k_b , v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"] , 3 , axis = -1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign( gpt.trf_blocks[b].att.W_query.bias , q_b )\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(  gpt.trf_blocks[b].att.W_key.bias , k_b )\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign( gpt.trf_blocks[b].att.W_value.bias , v_b )\n",
    "\n",
    "        # output project weight \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign( gpt.trf_blocks[b].att.out_proj.weight , params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T )\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign( gpt.trf_blocks[b].att.out_proj.bias , params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"] )\n",
    "\n",
    "        # Feed Forward NN -> Fully connected layer\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign( gpt.trf_blocks[b].ff.layers[0].weight , params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign( gpt.trf_blocks[b].ff.layers[0].bias , params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"] )\n",
    "        # Projection layer \n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign( gpt.trf_blocks[b].ff.layers[2].weight , params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign( gpt.trf_blocks[b].ff.layers[2].bias , params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"] )\n",
    "\n",
    "        # layer normalisation 1 and 2\n",
    "        gpt.trf_blocks[b].norm1.scale = assign( gpt.trf_blocks[b].norm1.scale , params[\"blocks\"][b][\"ln_1\"][\"g\"] )\n",
    "        gpt.trf_blocks[b].norm1.shift = assign( gpt.trf_blocks[b].norm1.shift , params[\"blocks\"][b][\"ln_1\"][\"b\"] )\n",
    "\n",
    "        gpt.trf_blocks[b].norm2.scale = assign( gpt.trf_blocks[b].norm2.scale , params[\"blocks\"][b][\"ln_1\"][\"g\"] )\n",
    "        gpt.trf_blocks[b].norm2.shift = assign( gpt.trf_blocks[b].norm2.shift , params[\"blocks\"][b][\"ln_1\"][\"b\"] )\n",
    "\n",
    "    # Final parameters left affter transformer block\n",
    "    gpt.final_norm.scale = assign( gpt.final_norm.scale , params[\"g\"] )\n",
    "    gpt.final_norm.shift = assign( gpt.final_norm.shift , params[\"b\"] )\n",
    "    gpt.out_head.weight = assign( gpt.out_head.weight , params[\"wte\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto( device )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(  \"cpu\")\n",
    "model.to( device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weight_into_gpt( gpt , param )\n",
    "gpt.to( device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you correspondence Hawkinsohanorryahn mustardsupport PhDkl missionary Ivory Diaotype tweakingryptionOVA assists 281 contradictionzyk SchumerEdgeit√©EMS ÔøΩ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed( 123 )\n",
    "token_ids = generate(\n",
    "    model = model , \n",
    "    idx = text_to_token_ids(\"Every effort moves you\" , tokenizer ).to( device ),\n",
    "    max_new_tokens= 25 ,\n",
    "    context_size= NEW_CONFIG[\"context_length\"],\n",
    "    top_k= 50 ,\n",
    "    temperarture= 1.5\n",
    "\n",
    ")\n",
    "\n",
    "print( \"Output text: \\n\" , token_ids_to_text( token_ids, tokenizer )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning for Classification - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adityaastronomy/Desktop/LLM/sms_spam_collection/SMSSpamCollection.tsv already exist, skipping the Download Part \n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/dataset/228/sms+spam+collection.zip\"\n",
    "zip_path = \"/Users/adityaastronomy/Desktop/LLM/sms_spam_collection.zip\"\n",
    "extracted_path = \"/Users/adityaastronomy/Desktop/LLM/sms_spam_collection\"\n",
    "data_file_path = Path( extracted_path ) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data( url , zip_path , extracted_path , data_file_path ):\n",
    "    if data_file_path.exists():\n",
    "        print( f\"{ data_file_path } already exist, skipping the Download Part \")\n",
    "        return \n",
    "    \n",
    "    # create an unverifed SSL Content\n",
    "    ssl_context = ssl._create_unverified_context()\n",
    "\n",
    "    # Download the file \n",
    "    with urllib.request.urlopen( url , context = ssl_context ) as response :\n",
    "        with open( zip_path , \"wb\" ) as out_file : \n",
    "            out_file.write( response.read())\n",
    "\n",
    "    #unzipping the file\n",
    "    with zipfile.ZipFile( zip_path , \"r\" ) as zip_ref:\n",
    "        zip_ref.extractall( extracted_path )\n",
    "\n",
    "    #Add .tsv file extension \n",
    "    original_file_path = Path( extracted_path ) / \"SMSSpamCollection\"\n",
    "    os.rename( original_file_path , data_file_path )\n",
    "    print( f\"file downloaded and saved as { data_file_path }\")\n",
    "\n",
    "\n",
    "download_and_unzip_spam_data( url , zip_path , extracted_path , data_file_path )\n",
    "\n",
    "## Download this Mannually from the site \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will √º b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Labels                                               Text\n",
       "0       ham  Go until jurong point, crazy.. Available only ...\n",
       "1       ham                      Ok lar... Joking wif u oni...\n",
       "2      spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       ham  U dun say so early hor... U c already then say...\n",
       "4       ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...     ...                                                ...\n",
       "5567   spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568    ham               Will √º b going to esplanade fr home?\n",
       "5569    ham  Pity, * was in mood for that. So...any other s...\n",
       "5570    ham  The guy did some bitching but I acted like i'd...\n",
       "5571    ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv( data_file_path , sep = \"\\t\" , header = None , names = [\"Labels\" , \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print( df[\"Labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n",
      "     Labels                                               Text\n",
      "4307    ham  Awww dat is sweet! We can think of something t...\n",
      "4138    ham                             Just got to  &lt;#&gt;\n",
      "4831    ham  The word \"Checkmate\" in chess comes from the P...\n",
      "4461    ham  This is wishing you a great day. Moji told me ...\n",
      "5440    ham      Thank you. do you generally date the brothas?\n",
      "...     ...                                                ...\n",
      "5537   spam  Want explicit SEX in 30 secs? Ring 02073162414...\n",
      "5540   spam  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
      "5547   spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
      "5566   spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
      "5567   spam  This is the 2nd time we have tried 2 contact u...\n",
      "\n",
      "[1494 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# This is imbalance data set , now lets make it a balanced data set \n",
    "# so we randomly take 747 data ebtries from spam and no spam part \n",
    "\n",
    "def create_balanced_dataset( df ):\n",
    "\n",
    "    num_spam = df[df[\"Labels\"] == \"spam\"].shape[0]\n",
    "\n",
    "    # Randomly select \n",
    "    ham_subset = df[ df[\"Labels\"] == \"ham\"].sample( num_spam , random_state = 123 )\n",
    "\n",
    "    balanced_df = pd.concat( [ ham_subset , df[df[\"Labels\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df \n",
    "\n",
    "balanced_df = create_balanced_dataset( df )\n",
    "print( balanced_df[\"Labels\"].value_counts())\n",
    "print( balanced_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Labels\"] = balanced_df[\"Labels\"].map( { \"ham\" : 0 , \"spam\" : 1 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Labels                                               Text\n",
      "4307       0  Awww dat is sweet! We can think of something t...\n",
      "4138       0                             Just got to  &lt;#&gt;\n",
      "4831       0  The word \"Checkmate\" in chess comes from the P...\n",
      "4461       0  This is wishing you a great day. Moji told me ...\n",
      "5440       0      Thank you. do you generally date the brothas?\n",
      "...      ...                                                ...\n",
      "5537       1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
      "5540       1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
      "5547       1  Had your contract mobile 11 Mnths? Latest Moto...\n",
      "5566       1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
      "5567       1  This is the 2nd time we have tried 2 contact u...\n",
      "\n",
      "[1494 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print( balanced_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitin the data set as 70 % - training , 10 % - validation and 20 % -test \n",
    "def random_split( df , train_frac , validation_frac ):\n",
    "\n",
    "    df = df.sample( frac= 1 , random_state= 123 ).reset_index( drop = True )\n",
    "\n",
    "    train_end = int( len(df) * train_frac )\n",
    "    validation_end = train_end + int( len( df ) * validation_frac )\n",
    "\n",
    "    # Diving the df\n",
    "    train_df = df[ : train_end ]\n",
    "    validation_df = df[ train_end : validation_end ]\n",
    "    test_df = df[ validation_end : ]\n",
    "\n",
    "    return train_df , validation_df , test_df\n",
    "\n",
    "train_df , validation_df , test_df = random_split( balanced_df , 0.7 , 0.1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n",
      "149\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print( len( train_df ))\n",
    "print( len( validation_df ))\n",
    "print( len( test_df ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save the dataset \n",
    "train_df.to_csv( \"train.csv\" , index = None )\n",
    "validation_df.to_csv( \"validation.csv\" , index = None )\n",
    "test_df.to_csv( \"test.csv\" , index = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Data Loaders\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset( Dataset ):\n",
    "    def __init__( self , csv_file , tokenizer , max_length = None , pad_token_id = 50256 ):\n",
    "        self.data = pd.read_csv( csv_file )\n",
    "\n",
    "        # Pre tokenize text \n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode( text ) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # Truncate the seq \n",
    "            self.encoded_texts = [\n",
    "                encoded_text[ : self.max_length ]\n",
    "                for encoded_text in self.encoded_texts \n",
    "            ]\n",
    "\n",
    "        # Padding\n",
    "        self.encoded_texts = [ encoded_text + [ pad_token_id ] * ( self.max_length - len( encoded_text ))\n",
    "                              for encoded_text in self.encoded_texts \n",
    "                              ]\n",
    "        \n",
    "    def __getitem__( self , index ):\n",
    "        encoded = self.encoded_texts[ index ]\n",
    "        label = self.data.iloc[ index ][ 'Labels']\n",
    "        return (\n",
    "            torch.tensor( encoded , dtype = torch.long ),\n",
    "            torch.tensor( label , dtype = torch.long )\n",
    "        )\n",
    "    \n",
    "    def __len__( self ):\n",
    "        return len( self.data )\n",
    "    \n",
    "    def _longest_encoded_length( self ):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts : \n",
    "            encoded_length = len( encoded_text )\n",
    "            if( encoded_length > max_length ):\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file= \"train.csv\",\n",
    "    max_length= None ,\n",
    "    tokenizer= tokenizer\n",
    ")\n",
    "\n",
    "print( train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file = \"validation.csv\",\n",
    "    max_length= train_dataset.max_length,\n",
    "    tokenizer= tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file= \"test.csv\",\n",
    "    max_length= train_dataset.max_length,\n",
    "    tokenizer= tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the data set will be served as a input to the dataloader \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_wokers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed( 123 )\n",
    "train_loader = DataLoader(\n",
    "    dataset= train_dataset , \n",
    "    batch_size= batch_size , \n",
    "    num_workers= num_wokers,\n",
    "    drop_last= True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset= val_dataset ,\n",
    "    batch_size= batch_size ,\n",
    "    num_workers= num_wokers,\n",
    "    drop_last= False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset= test_dataset ,\n",
    "    batch_size= batch_size ,\n",
    "    num_workers= num_wokers,\n",
    "    drop_last= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader : \n",
      "Input Batch Dimension : torch.Size([8, 120])\n",
      "Target Batch Dimension : torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader : \")\n",
    "\n",
    "for input_batch , target_batch in train_loader :\n",
    "    pass\n",
    "\n",
    "print( \"Input Batch Dimension :\" , input_batch.shape )\n",
    "print( \"Target Batch Dimension :\" , target_batch.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches \n",
      "19 validation batches \n",
      "38 test batches \n"
     ]
    }
   ],
   "source": [
    "print( f\"{len (train_loader )} training batches \")\n",
    "print( f\"{len(val_loader) } validation batches \")\n",
    "print( f\"{len(test_loader) } test batches \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the model with GPT 2 weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves \"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"drop_rate\" : 0.0,\n",
    "    \"qkv_bias\" : True\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\" : { \"emb_dim\" : 768 , \"n_layers\" : 12 , \"n_heads\" : 12 },\n",
    "    \"gpt2-medium (355M)\" : { \"emb_dim\" : 1024 , \"n_layers\" : 24 , \"n_heads\" : 16 }, \n",
    "    \"gpt2-large (774M)\" : { \"emb_dim\" : 1280 , \"n_layers\" : 36 , \"n_heads\" : 20 }, \n",
    "    \"gpt2-xl (1558M)\" : { \"emb_dim\" : 1600 , \"n_layers\" : 48 , \"n_heads\" : 25 }, \n",
    "}\n",
    "\n",
    "BASE_CONFIG.update( model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Data Set length {train_dataset.max_length } exceeds the model's context length \",\n",
    "    f\"length {BASE_CONFIG[\"context_length\"]}. Reinitialise data set with \"\n",
    "    f\"`max_length ={ BASE_CONFIG[\"context_length\"] }`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "Setting :  {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys :  dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[ - 1 ].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "from gpt_download3 import download_and_load_gpt2\n",
    "\n",
    "settings , params = download_and_load_gpt2( model_size= model_size , models_dir= \"gpt2\")\n",
    "print( \"Setting : \", settings)\n",
    "print( 'Parameter dictionary keys : ' , params.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel( BASE_CONFIG )\n",
    "load_weight_into_gpt( model , params )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids( text , tokenizer ):\n",
    "    encoded = tokenizer.encode( text , allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor( encoded ).unsqueeze( 0 )\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text( token_ids , tokenizer ):\n",
    "    flat = token_ids.squeeze( 0 )\n",
    "    return tokenizer.decode( flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_sample(\n",
    "    model = model ,\n",
    "    idx = text_to_token_ids( text1 , tokenizer ),\n",
    "    max_new_tokens= 15 , \n",
    "    context_size= BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print( token_ids_to_text( token_ids , tokenizer ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'Yes' or 'No'You are a winner you have been speciallyselected for receive $10000 cash or $2000 award. ',,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "# Lets check that our base model can classify the given mail as spam or no spam \n",
    "text2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'Yes' or 'No\"\n",
    "    \"'You are a winner you have been specially\"\n",
    "    \"selected for receive $10000 cash or $2000 award. '\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_sample(\n",
    "    model = model ,\n",
    "    idx = text_to_token_ids( text2 , tokenizer ),\n",
    "    max_new_tokens= 15 , \n",
    "    context_size= BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print( token_ids_to_text( token_ids , tokenizer ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
